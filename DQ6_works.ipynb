{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1boUq5x6GK4jF1xIVjciy6l0i5jD6O6qp",
      "authorship_tag": "ABX9TyMVtUDYcJApBnKx47UAaf6y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsdojocausuri/MachineLearning/blob/master/DQ6_works.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textdistance\n",
        "!pip install rapidfuzz"
      ],
      "metadata": {
        "id": "t8UVztfweN32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAdjnxSId1CE",
        "outputId": "ad0dd939-3d6a-4575-97c1-18edc5e5a58c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clustering and merging completed successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rapidfuzz import fuzz\n",
        "import textdistance\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Clean and standardize text inputs\n",
        "def clean_text(text):\n",
        "    return text.strip().upper() if pd.notna(text) else \"\"\n",
        "\n",
        "# Calculate match score using different algorithms\n",
        "def calculate_match_score(value1, value2, algorithm):\n",
        "    if algorithm == \"fuzz\":\n",
        "        return fuzz.ratio(value1, value2) / 100\n",
        "    elif algorithm == \"token_sort\":\n",
        "        return fuzz.token_sort_ratio(value1, value2) / 100\n",
        "    elif algorithm == \"jaro_winkler\":\n",
        "        return textdistance.jaro_winkler(value1, value2)\n",
        "    elif algorithm == \"levenshtein\":\n",
        "        return textdistance.levenshtein.normalized_similarity(value1, value2)\n",
        "    return 0.0\n",
        "\n",
        "# Load CSV file\n",
        "def load_csv(file_path):\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Generate a blocking key with more specificity\n",
        "def generate_block_key(df):\n",
        "    return (\n",
        "        df['Producername'].str[:5] + \"_\" +\n",
        "        df['Subproducername'].str[:5] + \"_\" +\n",
        "        df['subproduceraddress'].str[:5]\n",
        "    )\n",
        "\n",
        "# Assign clusters within each block with a global cluster ID counter\n",
        "def assign_clusters_block(df_block, config, final_threshold, cluster_id):\n",
        "    df_block = df_block.reset_index(drop=True)\n",
        "    df_block['ClusterId'] = np.nan\n",
        "    df_block['Producername_Score'] = np.nan\n",
        "    df_block['Subproducername_Score'] = np.nan\n",
        "    df_block['Address_Score'] = np.nan\n",
        "    df_block['Weighted_Score'] = np.nan\n",
        "\n",
        "    for i in range(len(df_block)):\n",
        "        if pd.isna(df_block.at[i, \"ClusterId\"]):\n",
        "            df_block.at[i, \"ClusterId\"] = cluster_id\n",
        "            for j in range(i + 1, len(df_block)):\n",
        "                producer_score = 0\n",
        "                subproducer_score = 0\n",
        "                address_score = 0\n",
        "                weighted_score_sum = 0\n",
        "                total_weight = 0\n",
        "\n",
        "                # Calculate match scores for each configured column\n",
        "                for conf in config:\n",
        "                    column = conf['column']\n",
        "                    algorithm = conf['algorithm']\n",
        "                    threshold = conf['threshold']\n",
        "                    weight = conf['weight']\n",
        "\n",
        "                    # Ensure both values are non-null before comparing\n",
        "                    if pd.notna(df_block.at[i, column]) and pd.notna(df_block.at[j, column]):\n",
        "                        score = calculate_match_score(df_block.at[i, column], df_block.at[j, column], algorithm)\n",
        "                    else:\n",
        "                        score = 0\n",
        "\n",
        "                    if column == 'Producername':\n",
        "                        producer_score = score\n",
        "                    elif column == 'Subproducername':\n",
        "                        subproducer_score = score\n",
        "                    elif column == 'subproduceraddress':\n",
        "                        address_score = score\n",
        "\n",
        "                    if score >= threshold:\n",
        "                        weighted_score_sum += score * weight\n",
        "                    total_weight += weight\n",
        "\n",
        "                final_score = weighted_score_sum / total_weight if total_weight > 0 else 0\n",
        "\n",
        "                # Assign to cluster if final score is above the threshold\n",
        "                if final_score >= final_threshold:\n",
        "                    df_block.at[j, \"ClusterId\"] = cluster_id\n",
        "                    df_block.at[j, 'Producername_Score'] = producer_score\n",
        "                    df_block.at[j, 'Subproducername_Score'] = subproducer_score\n",
        "                    df_block.at[j, 'Address_Score'] = address_score\n",
        "                    df_block.at[j, 'Weighted_Score'] = final_score\n",
        "\n",
        "            cluster_id += 1\n",
        "\n",
        "    return df_block, cluster_id\n",
        "\n",
        "# Merge clusters into concatenated dataset with concatenated SubproducerID\n",
        "def merge_clusters_concatenated(df):\n",
        "    merged_data = []\n",
        "    for cluster_id in df['ClusterId'].unique():\n",
        "        cluster_df = df[df['ClusterId'] == cluster_id]\n",
        "        if not cluster_df.empty:\n",
        "            merged_row = {\n",
        "                'Producername': \" | \".join(cluster_df['Producername'].unique()),\n",
        "                'Subproducername': \" | \".join(cluster_df['Subproducername'].unique()),\n",
        "                'subproduceraddress': \" | \".join(cluster_df['subproduceraddress'].unique()),\n",
        "                'ProducerID': cluster_df['ProducerID'].max(),\n",
        "                'SubproducerID': \" | \".join(map(str, cluster_df['SubproducerID'].unique()))\n",
        "            }\n",
        "            merged_data.append(merged_row)\n",
        "    return pd.DataFrame(merged_data)\n",
        "\n",
        "# Merge clusters into golden record dataset, picking the max ProducerID and SubproducerID\n",
        "def merge_clusters_golden(df):\n",
        "    merged_data = []\n",
        "    for cluster_id in df['ClusterId'].unique():\n",
        "        cluster_df = df[df['ClusterId'] == cluster_id]\n",
        "        if not cluster_df.empty:\n",
        "            merged_row = {\n",
        "                'Producername': cluster_df.loc[cluster_df['Producername'].apply(len).idxmax(), 'Producername'],\n",
        "                'Subproducername': cluster_df.loc[cluster_df['Subproducername'].apply(len).idxmax(), 'Subproducername'],\n",
        "                'subproduceraddress': cluster_df.loc[cluster_df['subproduceraddress'].apply(len).idxmax(), 'subproduceraddress'],\n",
        "                'ProducerID': cluster_df['ProducerID'].max(),\n",
        "                'SubproducerID': cluster_df['SubproducerID'].max()\n",
        "            }\n",
        "            merged_data.append(merged_row)\n",
        "    return pd.DataFrame(merged_data)\n",
        "\n",
        "# Main Function\n",
        "def main(file_path):\n",
        "    df = load_csv(file_path)\n",
        "    df['Producername'] = df['Producername'].apply(clean_text)\n",
        "    df['Subproducername'] = df['Subproducername'].apply(clean_text)\n",
        "    df['subproduceraddress'] = df['subproduceraddress'].apply(clean_text)\n",
        "\n",
        "    # Add Blocking Key\n",
        "    df['BlockKey'] = generate_block_key(df)\n",
        "\n",
        "    # Configuration for Matching\n",
        "    config = [\n",
        "        {'column': \"Producername\", 'algorithm': \"fuzz\", 'threshold': 0.9, 'weight': 0.6},\n",
        "        {'column': \"Subproducername\", 'algorithm': \"token_sort\", 'threshold': 0.85, 'weight': 0.8},\n",
        "        {'column': \"subproduceraddress\", 'algorithm': \"jaro_winkler\", 'threshold': 0.85, 'weight': 0.75}\n",
        "    ]\n",
        "    final_threshold = 0.85\n",
        "\n",
        "    blocks = df['BlockKey'].unique()\n",
        "    global_cluster_id = 0\n",
        "\n",
        "    # Process each block and assign clusters sequentially\n",
        "    results = []\n",
        "    for block in blocks:\n",
        "        block_result, global_cluster_id = assign_clusters_block(\n",
        "            df[df['BlockKey'] == block].copy(), config, final_threshold, global_cluster_id\n",
        "        )\n",
        "        results.append(block_result)\n",
        "\n",
        "    clustered_df = pd.concat(results, ignore_index=True)\n",
        "\n",
        "    # Generate merged datasets\n",
        "    merged_df_concat = merge_clusters_concatenated(clustered_df)\n",
        "    merged_df_golden = merge_clusters_golden(clustered_df)\n",
        "\n",
        "    # Save outputs to CSV files\n",
        "    clustered_df.to_csv(\"/content/sample_data/clustered_data.csv\", index=False)\n",
        "    merged_df_concat.to_csv(\"/content/sample_data/merged_concatenated.csv\", index=False)\n",
        "    merged_df_golden.to_csv(\"/content/sample_data/merged_golden.csv\", index=False)\n",
        "\n",
        "    print(\"Clustering and merging completed successfully!\")\n",
        "\n",
        "# Run the program\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/sample_data/POCDQ.csv\"\n",
        "    main(file_path)\n"
      ]
    }
  ]
}